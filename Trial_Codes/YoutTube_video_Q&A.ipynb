{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bc776d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q&A with Youtube Transcript using LangChain and HuggingFace\n",
    "\n",
    "# Starting with a MVP (Minimum Viable Product) - Essential features only [Enhancemebts to be added later]\n",
    "\n",
    "# Steps to be followed:\n",
    "# 1. Importing the required libraries\n",
    "# 2. Writing a function to fetch transcript from a given Youtube URL\n",
    "# 3. Setting up the language model and embedding model using HuggingFace API\n",
    "# 4. Text Splitting - Splitting the transcript into manageable chunks\n",
    "# 5. Creating a vector store to hold the embeddings\n",
    "# 6. Setting up the Retriever\n",
    "# 7. Creating a chain\n",
    "# 8. Creatin an UI using Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importing the required libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the transcript: 28785 characters\n",
      "First 100 characters of the transcript: [Music] translation it's done with a transform ER stat Quest hello I'm Josh starmer and welcome to s\n",
      "Last 100 characters of the transcript: t or a hoodie or just donate the links are in the description below alright until next time Quest on\n"
     ]
    }
   ],
   "source": [
    "# 2. Writing a function to fetch transcript from a given Youtube URL\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "def fetch_youtube_transcript(video_id: str) -> str:\n",
    "\n",
    "    ytt_api = YouTubeTranscriptApi()\n",
    "    fetched_transcript = ytt_api.fetch(video_id,\n",
    "                  languages=['en'])  # Specify the language of the transcript, default is English. Should try to translate it to English at a later stage if transcript is not available in English\n",
    "\n",
    "    # Converting the fetched transcript into a single string\n",
    "    transcript = \" \".join([snippet.text for snippet in fetched_transcript])\n",
    "    return transcript\n",
    "\n",
    "video_id = \"zxQyTK8quyY\" # Replace with your YouTube video ID, Write a function later to extract the video ID from the URL\n",
    "\n",
    "transcript = fetch_youtube_transcript(video_id)\n",
    "\n",
    "# Checking the length of the transcript\n",
    "print(f\"Length of the transcript: {len(transcript)} characters\")\n",
    "\n",
    "# Printing the first and last 100 characters of the transcript\n",
    "print(f\"First 100 characters of the transcript: {transcript[:100]}\")\n",
    "print(f\"Last 100 characters of the transcript: {transcript[-100:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Setting up the language model and embedding model using HuggingFace API\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFaceEndpointEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load environment variables from a .env file\n",
    "\n",
    "# Setting up the language model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='deepseek-ai/DeepSeek-V3.1',\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Checking the model\n",
    "# result = model.invoke(\"What is the capital of India\")\n",
    "\n",
    "# print(result.content)\n",
    "\n",
    "# Setting up the embedding model\n",
    "embedding_model = HuggingFaceEndpointEmbeddings(\n",
    "    repo_id='sentence-transformers/all-MiniLM-L6-v2',\n",
    "    task=\"feature-extraction\"\n",
    ")\n",
    "# # Checking the embedding model\n",
    "# embedding = embedding_model.embed_query(\"Hello, how are you?\")\n",
    "# print(f\"Embedding length: {len(embedding)}\")\n",
    "# print(f\"Embedding: {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 34\n",
      "First few chunks: page_content='[Music] translation it's done with a transform ER stat Quest hello I'm Josh starmer and welcome to statquest today we're going to talk about Transformer neural networks and they're going to be clearly explained Transformers are more fun when you build them in the cloud with lightning bam right now people are going bonkers about something called chat GPT for example our friend statsquatch might type something into chat GPT like right and awesome song in the style of statquest translation it's done with a transform ER anyway there's a lot to be said about how chat GPT works but fundamentally it is based on something called a Transformer so in this stat Quest we're going to show you how a Transformer works one step at a time specifically we're going to focus on how a Transformer neural network can translate a simple English sentence let's go into Spanish vamos now since a Transformer is a type of neural network and neural networks usually only have numbers for input values the first thing'\n",
      "page_content='into Spanish vamos now since a Transformer is a type of neural network and neural networks usually only have numbers for input values the first thing we need to do is find a way to turn the input and output words into numbers there are a lot of ways to convert words into numbers but for neural networks one of the most commonly used methods is called word embedding the main idea of word embedding is to use a relatively simple neural network that has one input for every word and symbol in the vocabulary that you want to use in this case we have a super simple vocabulary that allows us to input short phrases like let's go and to go and we have an input for this symbol EOS which stands for end of sentence or end of sequence because the vocabulary can be a mix of words word fragments and symbols we call each input a token the inputs are then connected to something called an activation function and in this example we have two activation functions and each connection multiplies the input'\n",
      "page_content='connected to something called an activation function and in this example we have two activation functions and each connection multiplies the input value by something called a weight hey Josh where do these numbers come from great question Squatch and we'll answer it in just a bit for now let's just see how we convert the word let's into numbers first we put a 1 into the input for let's and then put zeros into all of the other inputs now we multiply the inputs by their weights on the connections to the activation functions for example the input for let's is one so we multiply 1.87 by 1 to get 1.87 going to the activation function on the left and we multiply 0.09 by 1 to get 0.09 going to the activation function on the right in contrast if the input value for the word 2 is 0 then we multiply negative 1.45 by 0 to get 0 going to the activation function on the left and we multiply 1.50 by 0 to get 0 going to the activation function on the right in other words when an input value is 0 then'\n"
     ]
    }
   ],
   "source": [
    "# 4. Text Splitting - Splitting the transcript into manageable chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "\n",
    "chunks = text_splitter.create_documents([transcript])\n",
    "\n",
    "# print(len(texts))\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"First few chunks: {chunks[0]}\")\n",
    "print(f\"{chunks[1]}\")\n",
    "print(f\"{chunks[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ec7a6cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors in the vector store: 34\n",
      "First vector shape: (384,)\n",
      "First vector: [-1.39453992e-01 -5.08732013e-02  2.45066546e-02 -5.01772389e-02\n",
      " -4.73994203e-02  1.66481528e-02  8.81869271e-02  5.52349910e-02\n",
      "  1.00835539e-01 -3.35111506e-02 -3.68931983e-03  6.09022416e-02\n",
      " -2.07618438e-02  1.50899962e-02  2.31710915e-02 -2.51774266e-02\n",
      " -2.41745561e-02  2.98863649e-02 -9.63017568e-02 -4.59887348e-02\n",
      "  9.84680057e-02  1.63164362e-02 -2.84353625e-02  3.84797715e-02\n",
      "  8.52074549e-02  5.33244908e-02 -2.03639455e-03 -3.83457989e-02\n",
      "  2.58153630e-03 -2.88394317e-02 -4.28568535e-02  5.61851449e-02\n",
      " -2.67245974e-02  6.48040995e-02 -1.64009333e-01  2.52955537e-02\n",
      " -3.29103023e-02 -5.35792187e-02 -5.45955934e-02  2.41573937e-02\n",
      " -1.77636202e-02 -6.43353909e-02  4.56412062e-02 -3.78277414e-02\n",
      " -9.98049509e-04 -2.37403624e-02 -5.18258959e-02 -3.74019369e-02\n",
      " -5.49683049e-02  1.54513046e-02 -5.85036948e-02 -5.82067594e-02\n",
      " -2.87286509e-02  1.48616314e-01 -2.43626963e-02  9.22958255e-02\n",
      "  2.28577107e-02  2.44925879e-02  7.25811422e-02  1.04064820e-02\n",
      " -3.80354822e-02  1.42655931e-02 -1.57904401e-02 -2.45162845e-02\n",
      " -3.88076566e-02 -1.91270076e-02  3.69074941e-02  4.42054719e-02\n",
      " -6.29149303e-02 -1.15494814e-03 -2.61943340e-02 -2.87470855e-02\n",
      " -2.39812732e-02  7.53526911e-02  9.74674150e-02 -9.96703748e-03\n",
      "  1.66930910e-02 -6.44254312e-02 -1.58275776e-02  5.20090833e-02\n",
      "  1.16348125e-01  2.42331401e-02  5.57089150e-02 -3.81929148e-03\n",
      " -2.88025886e-02 -1.17024882e-02 -1.61899161e-02  1.75825320e-02\n",
      " -1.89755466e-02 -1.05231376e-02 -1.02626622e-01 -2.24162322e-02\n",
      "  1.13207027e-02  5.55071756e-02  3.21251936e-02  3.33749056e-02\n",
      " -8.64155665e-02 -1.05932923e-02  1.03589548e-02  1.87368151e-02\n",
      "  4.03844826e-02  1.04431463e-02  5.96561767e-02 -6.08764216e-02\n",
      " -1.14445925e-01 -6.19463548e-02  3.31583200e-03  7.41512999e-02\n",
      " -2.29391959e-02 -4.51067239e-02 -5.84470760e-03 -3.72774713e-02\n",
      "  9.56592802e-03 -4.67209257e-02 -1.93904396e-02 -3.28924432e-02\n",
      " -4.07389272e-03  1.78981945e-02 -5.38220927e-02  3.21423489e-04\n",
      "  4.81571667e-02  2.40676086e-02 -6.07822090e-02  3.29335481e-02\n",
      " -1.55206199e-03  4.75304238e-02 -3.38105895e-02  5.48636091e-33\n",
      "  3.98518378e-03  2.06306335e-02  5.37391156e-02  8.12290236e-02\n",
      "  2.95924414e-02 -4.59176395e-03 -3.68399471e-02 -1.16039952e-03\n",
      "  3.30058113e-02 -1.61372591e-02 -2.80134380e-02  8.67538527e-02\n",
      " -3.11367586e-02  3.83877233e-02 -5.24943545e-02 -5.11873625e-02\n",
      " -9.25033446e-03  1.50624104e-02  5.22256196e-02 -5.66594712e-02\n",
      "  6.62260354e-02  5.45602664e-02  4.37297821e-02 -2.28686891e-02\n",
      " -4.73810732e-02  2.31707636e-02 -5.78542380e-03 -1.07684791e-01\n",
      "  1.90355983e-02 -1.29055157e-02 -9.75375623e-02 -1.04652718e-02\n",
      " -3.50874290e-02 -2.73747798e-02  5.50109409e-02 -5.99119514e-02\n",
      "  8.71158019e-02 -7.79752135e-02 -1.92227215e-02 -6.67080879e-02\n",
      " -1.71738993e-02  6.98422734e-03 -9.00959820e-02 -1.71985943e-02\n",
      "  2.89596692e-02  4.25565280e-02  1.53205683e-02  4.64527309e-03\n",
      "  5.32422028e-02  2.56337672e-02 -4.95696627e-02 -1.05255684e-02\n",
      " -6.31926209e-02  4.58571576e-02  1.11245915e-01  9.72180068e-02\n",
      " -7.26269139e-03  2.74042543e-02  9.76001006e-03  2.14563813e-02\n",
      " -6.80796755e-03  4.70704623e-02  3.64843644e-02 -4.82102633e-02\n",
      "  1.40413120e-02 -2.60358788e-02 -1.86722889e-03 -1.83402784e-02\n",
      "  7.15890229e-02 -4.64580627e-03 -4.44146693e-02 -1.63821820e-02\n",
      " -6.43001646e-02  1.19923055e-02  1.70434676e-02  5.09212464e-02\n",
      " -1.51108755e-02  5.89668006e-02  3.89653700e-03  5.02904467e-02\n",
      " -1.19019859e-01 -2.98809242e-02 -9.63295158e-03  1.64930336e-02\n",
      "  1.11303339e-03  8.76404643e-02  8.76809657e-02 -5.83729558e-02\n",
      " -4.46019918e-02  1.30936084e-02 -9.73336399e-02  3.22616622e-02\n",
      " -5.69336489e-02 -2.21814867e-02  2.40665823e-02 -6.40430628e-33\n",
      " -2.96487324e-02  1.67734828e-02 -5.10099716e-02  1.12392239e-01\n",
      "  1.56004273e-03 -3.48241627e-02  4.28791717e-02  5.36519960e-02\n",
      "  9.19598714e-02  2.27186475e-02 -1.63181610e-02 -7.34777078e-02\n",
      " -3.97874117e-02 -8.28765407e-02  5.64139448e-02 -8.47870335e-02\n",
      " -4.01155427e-02 -3.08459513e-02 -4.97261174e-02  8.08658004e-02\n",
      " -2.70273313e-02  7.53783211e-02 -1.17734112e-01  3.23551670e-02\n",
      " -7.67145529e-02 -4.31491882e-02 -3.30625921e-02  5.68075925e-02\n",
      "  5.10886833e-02  3.74923721e-02  1.53565798e-02 -6.12641275e-02\n",
      " -4.40085381e-02 -2.79161660e-03 -8.05562213e-02 -1.41889695e-02\n",
      "  1.16963945e-01 -1.11689093e-02 -5.47541715e-02 -9.77401901e-03\n",
      " -2.04291847e-03 -4.59687077e-02 -2.21590209e-03  8.44972767e-03\n",
      " -3.87227796e-02  3.51829156e-02 -1.69524364e-02 -8.75848159e-03\n",
      " -4.00985144e-02 -1.10814106e-02  1.20086744e-01 -4.05704975e-02\n",
      " -7.05095455e-02 -3.36080305e-02 -3.59227285e-02 -1.29942819e-01\n",
      "  4.91104573e-02 -8.58901069e-02 -4.65475880e-02 -3.36889997e-02\n",
      "  1.78854465e-02 -4.54001948e-02  7.68220574e-02 -9.28662419e-02\n",
      " -5.04574552e-02 -2.07688566e-02 -7.82274920e-03 -5.76253235e-02\n",
      "  7.93529898e-02  5.46370558e-02  9.46041122e-02 -1.48879420e-02\n",
      "  2.47171540e-02 -4.71751615e-02 -1.24193868e-02 -6.01547435e-02\n",
      " -4.59371414e-03 -6.15769550e-02  1.55725069e-02 -5.40535487e-02\n",
      " -4.36957330e-02 -3.49197462e-02  2.57966164e-02 -5.25290668e-02\n",
      "  6.43233657e-02  3.41131934e-03  7.77648315e-02  3.46578695e-02\n",
      "  5.74478917e-02  8.64789113e-02 -4.37967032e-02  8.38338956e-02\n",
      " -3.31773944e-02  6.19751774e-02 -2.45989785e-02 -5.31998587e-08\n",
      " -4.97914404e-02  3.40519920e-02 -3.84742096e-02  3.29672135e-02\n",
      " -1.00757484e-03 -2.77164802e-02  8.38200524e-02  4.91410121e-02\n",
      " -2.29413770e-02  4.37130295e-02  2.00473096e-05 -6.86659962e-02\n",
      "  2.29827929e-02 -2.59728208e-02  2.80207265e-02  7.98614398e-02\n",
      "  4.00065929e-02  1.48633406e-01 -7.39098294e-03  3.23091596e-02\n",
      " -5.04267286e-04  7.90077224e-02 -7.18495017e-03 -4.11776081e-03\n",
      "  2.53364514e-03  2.43338495e-02  5.83668286e-03 -3.46806124e-02\n",
      "  2.35827267e-02 -9.88468528e-03 -1.58104356e-02  2.28776056e-02\n",
      "  4.50926311e-02 -2.57192291e-02  1.62025765e-02 -2.22969782e-02\n",
      "  2.83580441e-02 -6.56350181e-02  2.29801498e-02 -2.97849160e-02\n",
      "  5.95466420e-02  2.58800271e-03 -9.66837928e-02  7.34271482e-02\n",
      " -5.35159856e-02 -6.41625077e-02  1.35799134e-02 -5.44783846e-02\n",
      " -5.86131774e-02  4.85933498e-02  5.25590740e-02 -3.44742574e-02\n",
      " -6.62012994e-02  1.42130600e-02  6.29165918e-02  4.23200876e-02\n",
      "  1.37368049e-02 -2.18237098e-02 -1.68922003e-02  7.12462887e-02\n",
      " -2.84360722e-02  1.34493917e-01  1.86688155e-02 -2.37068087e-02]\n"
     ]
    }
   ],
   "source": [
    "# 5. Creating a vector store to hold the embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "print(f\"Number of vectors in the vector store: {vectorstore.index.ntotal}\")\n",
    "\n",
    "# First vector shape\n",
    "print(f\"First vector shape: {vectorstore.index.reconstruct(0).shape}\")\n",
    "# First vector\n",
    "print(f\"First vector: {vectorstore.index.reconstruct(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c6b6f7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='577f29f9-41e7-4f9d-952d-78e8d94dce8f', metadata={}, page_content=\"neural networks with hidden layers to both the encoder and decoder bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the statquest PDF study guides in my book the stat Quest Illustrated guide to machine learning at stackwest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stack Quest and want to see more please subscribe and if you want to support stackquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time Quest on\"),\n",
       " Document(id='62af0744-20b3-4cbc-91dc-32992e276013', metadata={}, page_content=\"to pizza or potentially it could refer to the word oven Josh I've heard of good tasting pizza but never a good tasting oven I know Squatch that's why it's important that the Transformer correctly Associates the word it with pizza the good news is that Transformers have something called self-attention which is a mechanism to correctly associate the word ID with the word Pizza in general terms self-attention works by seeing how similar each word is to all of the words in the sentence including itself for example self-attention calculates the similarity between the first word the and all of the words in the sentence including itself and self-attention calculates these similarities for every word in the sentence once the similarities are calculated they are used to determine how the Transformer encodes each word for example if you looked at a lot of sentences about pizza and the word ID was more commonly associated with pizza than oven then the similarity score for pizza will cause it to\"),\n",
       " Document(id='b365c6b9-046d-4985-be55-706bfc5fffea', metadata={}, page_content=\"represent the word order come from a sequence of alternating sine and cosine squiggles each squiggle gives a specific position values for each word's embeddings for example the y-axis values on the green squiggle give us position encoding values for the first embeddings for each word specifically for the first word which has an x-axis coordinate all the way to the left of the green squiggle the position value for the first embedding is the y-axis coordinate zero the position value for the second embedding comes from the orange squiggle and the y-axis coordinate on the orange squiggle that corresponds to the first word is one likewise the blue squiggle which is more spread out than the first two squiggles gives us the position value for the third embedding value which for the first word is zero lastly the red squiggle gives us the position value for the fourth embedding which for the first word is one thus the position values for the first word come from the corresponding y-axis\"),\n",
       " Document(id='2201a61a-dfd2-4e43-b79a-cb4b8b90399b', metadata={}, page_content=\"[Music] translation it's done with a transform ER stat Quest hello I'm Josh starmer and welcome to statquest today we're going to talk about Transformer neural networks and they're going to be clearly explained Transformers are more fun when you build them in the cloud with lightning bam right now people are going bonkers about something called chat GPT for example our friend statsquatch might type something into chat GPT like right and awesome song in the style of statquest translation it's done with a transform ER anyway there's a lot to be said about how chat GPT works but fundamentally it is based on something called a Transformer so in this stat Quest we're going to show you how a Transformer works one step at a time specifically we're going to focus on how a Transformer neural network can translate a simple English sentence let's go into Spanish vamos now since a Transformer is a type of neural network and neural networks usually only have numbers for input values the first thing\"),\n",
       " Document(id='24f24058-e271-4a4c-8beb-446af12a86a6', metadata={}, page_content=\"note there's a lot more to be said about the soft Max function so if you're interested check out the quest anyway because we want 100 of the word let's to encode let's we create two more values that will cleverly call values to represent the word let's and scale the values that represent let's by 1.0 then we create two values to represent the word go and scale those values by 0.0 lastly we add the scaled values together and these sums which combine separate encodings for both input words let's and go relative to their similarity to Let's are the self-attention values for leads bam now that we have self-attention values for the word let's it's time to calculate them for the word go the good news is that we don't need to recalculate the keys and values instead all we need to do is create the query that represents the word go and do the math by first calculating the similarity scores between the new query and the keys and then run the similarity scores through a softmax and then scale\"),\n",
       " Document(id='1ddd9b17-4c1b-417f-a2ab-088836b6cd66', metadata={}, page_content=\"is the Spanish translation for Let's Go triple boom no not yet so far the translation is correct but the decoder doesn't stop until it outputs an EOS token so let's consolidate our diagrams and plug the translated word vamos into a copy of the decoder's embedding layer and do the math first we get the word embeddings for vamos then we add the positional encoding now we calculate self-attention values for vamos using the exact same weights that we used for the EOS token now add the residual connections and calculate the encoder decoder attention using the same sets of Weights that we used for the EOS token now we add more residual connections lastly we run the values that represent vamos through the same fully connected layer and softmax that we used for the EOS token and the second output from the decoder is eos so we are done decoding triple bam at long last we've shown how a Transformer can encode a simple input phrase let's go and decode the encoding into the translated phrase of\"),\n",
       " Document(id='acbd1dca-3b4d-4c7c-88d9-80fd8115a41b', metadata={}, page_content=\"a lot of sentences about pizza and the word ID was more commonly associated with pizza than oven then the similarity score for pizza will cause it to have a larger impact on how the word ID is encoded by the Transformer bam and now that we know the main ideas of how self-attention Works let's look at the details so let's go back to our simple example where we had just added positional encoding to the words let's and go the first thing we do is multiply the position encoded values for the word let's by a pair of weights and we add those products together to get Negative 1.0 then we do the same thing with a different pair of weights to get 3.7 we do this twice because we started out with two position encoded values that represent the word leads and after doing the math two times we still have two values representing the word leads Josh I don't get it if we want two values to represent let's why don't we just use the two values we started with that's a great question Squatch and we'll\"),\n",
       " Document(id='ced6166e-ccb6-4d72-8c74-63a86364b543', metadata={}, page_content=\"and the original Transformer had 37 000 tokens and longer input and output phrases then in order to get their model to work they had to normalize the values after every step for example they normalize the values after positional encoding and after self-attention in both the encoder and the decoder also when we calculated attention values we used the dot product to calculate the similarities but you can use whatever similarity function you want in the original Transformer manuscript they calculated the similarities with a DOT product divided by the square root of the number of embedding values per token just like with scaling the values after each step they found that scaling the dot product helped encode and decode long and complicated phrases lastly to give a Transformer more weights and biases to fit to complicated data you can add additional neural networks with hidden layers to both the encoder and decoder bam now it's time for some Shameless self-promotion if you want to review\")]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Setting up the Retriever using multi-query retrieval with Maximal Marginal Relevance (MMR) as base-retriever\n",
    "\n",
    "from langchain.retrievers import MultiQueryRetriever\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "\n",
    "\n",
    "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever,\n",
    "    llm=model)\n",
    "\n",
    "# Checking the retriever on a sample query\n",
    "multiquery_retriever.invoke(\"Most important points of the video?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "12a2978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Creating a chain by combining the retriever and the language model\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a knowledgeable assistant answering questions about a YouTube video.\n",
    "\n",
    "Use ONLY the information from the transcript context below.\n",
    "- If the context does not contain the answer, reply with: \"I don't know based on the transcript.\"\n",
    "- Do NOT use outside knowledge.\n",
    "- Prefer concise, factual answers.\n",
    "- If multiple relevant points exist, summarize them in bullet points.\n",
    "- If transcript timestamps are available in the context, include them in your answer.\n",
    "\n",
    "---\n",
    "Transcript context:\n",
    "{context}\n",
    "---\n",
    "Question: {question}\n",
    "\n",
    "\"\"\",\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "01e057f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='7d89403a-4fa9-449e-aafa-f95f8675730d', metadata={}, page_content=\"of how words are related within a sentence however since We're translating a sentence we also need to keep track of the relationships between the input sentence and the output for example if the input sentence was don't eat the delicious looking and smelling pizza then when translating it's super important to keep track of the very first word don't if the translation focuses on other parts of the sentence and omits the don't then we'll end up with eat the delicious looking and smelling Pizza and these two sentences have completely opposite meanings so it's super important for the decoder to keep track of the significant words in the input so the main idea of encoder decoder attention is to allow the decoder to keep track of the significant words in the input now that we know the main idea behind encoder decoder attention here are the details first to give us a little more room let's consolidate the math and the diagrams now just like we did for self-attention we create two new values\"),\n",
       " Document(id='2201a61a-dfd2-4e43-b79a-cb4b8b90399b', metadata={}, page_content=\"[Music] translation it's done with a transform ER stat Quest hello I'm Josh starmer and welcome to statquest today we're going to talk about Transformer neural networks and they're going to be clearly explained Transformers are more fun when you build them in the cloud with lightning bam right now people are going bonkers about something called chat GPT for example our friend statsquatch might type something into chat GPT like right and awesome song in the style of statquest translation it's done with a transform ER anyway there's a lot to be said about how chat GPT works but fundamentally it is based on something called a Transformer so in this stat Quest we're going to show you how a Transformer works one step at a time specifically we're going to focus on how a Transformer neural network can translate a simple English sentence let's go into Spanish vamos now since a Transformer is a type of neural network and neural networks usually only have numbers for input values the first thing\"),\n",
       " Document(id='577f29f9-41e7-4f9d-952d-78e8d94dce8f', metadata={}, page_content=\"neural networks with hidden layers to both the encoder and decoder bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the statquest PDF study guides in my book the stat Quest Illustrated guide to machine learning at stackwest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stack Quest and want to see more please subscribe and if you want to support stackquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time Quest on\"),\n",
       " Document(id='24f24058-e271-4a4c-8beb-446af12a86a6', metadata={}, page_content=\"note there's a lot more to be said about the soft Max function so if you're interested check out the quest anyway because we want 100 of the word let's to encode let's we create two more values that will cleverly call values to represent the word let's and scale the values that represent let's by 1.0 then we create two values to represent the word go and scale those values by 0.0 lastly we add the scaled values together and these sums which combine separate encodings for both input words let's and go relative to their similarity to Let's are the self-attention values for leads bam now that we have self-attention values for the word let's it's time to calculate them for the word go the good news is that we don't need to recalculate the keys and values instead all we need to do is create the query that represents the word go and do the math by first calculating the similarity scores between the new query and the keys and then run the similarity scores through a softmax and then scale\"),\n",
       " Document(id='ced6166e-ccb6-4d72-8c74-63a86364b543', metadata={}, page_content=\"and the original Transformer had 37 000 tokens and longer input and output phrases then in order to get their model to work they had to normalize the values after every step for example they normalize the values after positional encoding and after self-attention in both the encoder and the decoder also when we calculated attention values we used the dot product to calculate the similarities but you can use whatever similarity function you want in the original Transformer manuscript they calculated the similarities with a DOT product divided by the square root of the number of embedding values per token just like with scaling the values after each step they found that scaling the dot product helped encode and decode long and complicated phrases lastly to give a Transformer more weights and biases to fit to complicated data you can add additional neural networks with hidden layers to both the encoder and decoder bam now it's time for some Shameless self-promotion if you want to review\")]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"is the topic of attention discussed in this video? if yes then what was discussed\"\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"of how words are related within a sentence however since We're translating a sentence we also need to keep track of the relationships between the input sentence and the output for example if the input sentence was don't eat the delicious looking and smelling pizza then when translating it's super important to keep track of the very first word don't if the translation focuses on other parts of the sentence and omits the don't then we'll end up with eat the delicious looking and smelling Pizza and these two sentences have completely opposite meanings so it's super important for the decoder to keep track of the significant words in the input so the main idea of encoder decoder attention is to allow the decoder to keep track of the significant words in the input now that we know the main idea behind encoder decoder attention here are the details first to give us a little more room let's consolidate the math and the diagrams now just like we did for self-attention we create two new values\\n\\n[Music] translation it's done with a transform ER stat Quest hello I'm Josh starmer and welcome to statquest today we're going to talk about Transformer neural networks and they're going to be clearly explained Transformers are more fun when you build them in the cloud with lightning bam right now people are going bonkers about something called chat GPT for example our friend statsquatch might type something into chat GPT like right and awesome song in the style of statquest translation it's done with a transform ER anyway there's a lot to be said about how chat GPT works but fundamentally it is based on something called a Transformer so in this stat Quest we're going to show you how a Transformer works one step at a time specifically we're going to focus on how a Transformer neural network can translate a simple English sentence let's go into Spanish vamos now since a Transformer is a type of neural network and neural networks usually only have numbers for input values the first thing\\n\\nneural networks with hidden layers to both the encoder and decoder bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the statquest PDF study guides in my book the stat Quest Illustrated guide to machine learning at stackwest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stack Quest and want to see more please subscribe and if you want to support stackquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time Quest on\\n\\nnote there's a lot more to be said about the soft Max function so if you're interested check out the quest anyway because we want 100 of the word let's to encode let's we create two more values that will cleverly call values to represent the word let's and scale the values that represent let's by 1.0 then we create two values to represent the word go and scale those values by 0.0 lastly we add the scaled values together and these sums which combine separate encodings for both input words let's and go relative to their similarity to Let's are the self-attention values for leads bam now that we have self-attention values for the word let's it's time to calculate them for the word go the good news is that we don't need to recalculate the keys and values instead all we need to do is create the query that represents the word go and do the math by first calculating the similarity scores between the new query and the keys and then run the similarity scores through a softmax and then scale\\n\\nand the original Transformer had 37 000 tokens and longer input and output phrases then in order to get their model to work they had to normalize the values after every step for example they normalize the values after positional encoding and after self-attention in both the encoder and the decoder also when we calculated attention values we used the dot product to calculate the similarities but you can use whatever similarity function you want in the original Transformer manuscript they calculated the similarities with a DOT product divided by the square root of the number of embedding values per token just like with scaling the values after each step they found that scaling the dot product helped encode and decode long and complicated phrases lastly to give a Transformer more weights and biases to fit to complicated data you can add additional neural networks with hidden layers to both the encoder and decoder bam now it's time for some Shameless self-promotion if you want to review\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fa516443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(retrieved_docs):\n",
    "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "  return context_text\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'context': multiquery_retriever | RunnableLambda(format_docs),\n",
    "    'question': RunnablePassthrough()\n",
    "})\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "main_chain = parallel_chain | prompt | model | parser\n",
    "\n",
    "result = main_chain.invoke('Can you summarize the video for a 15 year old?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "02b18459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the transcript, here's a summary for a 15-year-old:\n",
       "\n",
       "This video explains how Transformer neural networks work, focusing on how they can translate a simple English sentence into Spanish.\n",
       "\n",
       "*   **The Core Idea:** It introduces Transformers as the fundamental technology behind things like ChatGPT.\n",
       "*   **Step-by-Step Translation:** The video breaks down the translation process into steps, explaining how a neural network (which only understands numbers) handles words.\n",
       "*   **Key Concepts:**\n",
       "    *   **Positional Encoding:** How the Transformer keeps track of the order of words in a sentence using sine and cosine \"squiggles\" to give each word a unique position value.\n",
       "    *   **Self-Attention:** A mechanism that helps the Transformer understand the context of a sentence. For example, it calculates how similar each word is to all the others to correctly figure out what the word \"it\" refers to (like associating \"it\" with \"pizza\" and not \"oven\").\n",
       "    *   **Encoder-Decoder Attention:** This allows the part of the network generating the translation (the decoder) to keep track of and focus on the most important words from the original input sentence to ensure the meaning isn't lost (like remembering the word \"don't\").\n",
       "*   **Extra Details:** The video also mentions that real-world Transformers are more complex, using techniques like normalizing values and scaling calculations to handle long and complicated sentences."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "78bf7723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.49.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting altair!=5.4.0,!=5.4.1,<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from streamlit) (2.2.6)\n",
      "Requirement already satisfied: packaging<26,>=20 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from streamlit) (2.3.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from streamlit) (11.3.0)\n",
      "Collecting protobuf<7,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-6.32.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Using cached pyarrow-21.0.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from streamlit) (2.32.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from streamlit) (4.15.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from streamlit) (6.5.2)\n",
      "Collecting jinja2 (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jsonschema>=3.0 (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.27.1-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daman\\anaconda3\\envs\\yt_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.49.1-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 3.9/10.0 MB 21.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.0/10.0 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.5/10.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.0/10.0 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.6/10.0 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.6/10.0 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.9/10.0 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 5.5 MB/s  0:00:01\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-6.2.0-py3-none-any.whl (11 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading protobuf-6.32.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Downloading narwhals-2.5.0-py3-none-any.whl (407 kB)\n",
      "Using cached pyarrow-21.0.0-cp310-cp310-win_amd64.whl (26.2 MB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.27.1-cp310-cp310-win_amd64.whl (228 kB)\n",
      "Installing collected packages: watchdog, toml, smmap, rpds-py, pyarrow, protobuf, narwhals, MarkupSafe, click, cachetools, blinker, referencing, jinja2, gitdb, pydeck, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "\n",
      "   ---- -----------------------------------  2/20 [smmap]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   -------- -------------------------------  4/20 [pyarrow]\n",
      "   ---------- -----------------------------  5/20 [protobuf]\n",
      "   ---------- -----------------------------  5/20 [protobuf]\n",
      "   ------------ ---------------------------  6/20 [narwhals]\n",
      "   ------------ ---------------------------  6/20 [narwhals]\n",
      "   ------------ ---------------------------  6/20 [narwhals]\n",
      "   ------------ ---------------------------  6/20 [narwhals]\n",
      "   ------------ ---------------------------  6/20 [narwhals]\n",
      "   ---------------- -----------------------  8/20 [click]\n",
      "   -------------------- ------------------- 10/20 [blinker]\n",
      "   ------------------------ --------------- 12/20 [jinja2]\n",
      "   -------------------------- ------------- 13/20 [gitdb]\n",
      "   ---------------------------- ----------- 14/20 [pydeck]\n",
      "   ------------------------------ --------- 15/20 [jsonschema-specifications]\n",
      "   -------------------------------- ------- 16/20 [gitpython]\n",
      "   ---------------------------------- ----- 17/20 [jsonschema]\n",
      "   ------------------------------------ --- 18/20 [altair]\n",
      "   ------------------------------------ --- 18/20 [altair]\n",
      "   ------------------------------------ --- 18/20 [altair]\n",
      "   -------------------------------------- - 19/20 [streamlit]\n",
      "   -------------------------------------- - 19/20 [streamlit]\n",
      "   -------------------------------------- - 19/20 [streamlit]\n",
      "   -------------------------------------- - 19/20 [streamlit]\n",
      "   -------------------------------------- - 19/20 [streamlit]\n",
      "   -------------------------------------- - 19/20 [streamlit]\n",
      "   -------------------------------------- - 19/20 [streamlit]\n",
      "   -------------------------------------- - 19/20 [streamlit]\n",
      "   -------------------------------------- - 19/20 [streamlit]\n",
      "   -------------------------------------- - 19/20 [streamlit]\n",
      "   ---------------------------------------- 20/20 [streamlit]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.2 altair-5.5.0 blinker-1.9.0 cachetools-6.2.0 click-8.2.1 gitdb-4.0.12 gitpython-3.1.45 jinja2-3.1.6 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 narwhals-2.5.0 protobuf-6.32.1 pyarrow-21.0.0 pydeck-0.9.1 referencing-0.36.2 rpds-py-0.27.1 smmap-5.0.2 streamlit-1.49.1 toml-0.10.2 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5edc6e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 21:15:02.657 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 21:15:02.787 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\Daman\\anaconda3\\envs\\yt_env\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-09-15 21:15:02.788 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 21:15:02.788 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 21:15:02.789 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 21:15:02.790 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 21:15:02.791 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 21:15:02.791 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 21:15:02.792 Session state does not function when running a script without `streamlit run`\n",
      "2025-09-15 21:15:02.792 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 21:15:02.793 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-15 21:15:02.793 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# 8. Creatin an UI using Streamlit to interact with the chatbot\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"YouTube Video Q&A Chatbot\")\n",
    "question = st.text_input(\"Ask a question about the video:\")\n",
    "if question:\n",
    "    result = main_chain.invoke(question)\n",
    "    st.markdown(result)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408b21f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd8d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27330f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30ce0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf2f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b2aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ab2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153122de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defdf1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a24575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db801ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7d39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d731a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93eca59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ae560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49d17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2292d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd480b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542e8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d25170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849b62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf160f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120db4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
